import gc
import os
import pickle

import lightgbm as lgb
import numpy as np
import pandas as pd
import xgboost as xgb
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from pydantic import BaseModel
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

try:
    from .data import load_data, load_dataframe
except ImportError:
    from data import load_data, load_dataframe


class CacheFeatureTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, enable_cache: bool = True, cache_path: str = "./output/cache_feature.pkl") -> None:
        self.user_feature = None
        self.merchant_feature = None
        self.user_merchant_feature = None

        self.enable_cache = enable_cache
        self.cache_path = cache_path

    def fit(self, X, y=None):
        if self.enable_cache and os.path.exists(self.cache_path):
            print(f"â™»ï¸ ä»ç¼“å­˜åŠ è½½ç‰¹å¾: {self.cache_path}")
            with open(self.cache_path, "rb") as f:
                feature = pickle.load(f)
            self.user_feature = feature.get("user_feature", None)
            self.merchant_feature = feature.get("merchant_feature", None)
            self.user_merchant_feature = feature.get("user_merchant_feature", None)
            print("âœ… ç‰¹å¾åŠ è½½å®Œæˆ")
            return

        print("ğŸ”„ è®¡ç®—ç”¨æˆ·ã€å•†æˆ·åŠç”¨æˆ·-å•†æˆ·äº¤äº’ç‰¹å¾...")
        df = X.copy()
        df = self.explode(df)
        self.create_user_feature(df)
        self.create_merchant_feature(df)
        self.create_user_merchant_feature(df)
        print("âœ… ç‰¹å¾è®¡ç®—å®Œæˆ")

        if self.enable_cache:
            with open(self.cache_path, "wb") as f:
                pickle.dump(
                    {
                        "user_feature": self.user_feature,
                        "merchant_feature": self.merchant_feature,
                        "user_merchant_feature": self.user_merchant_feature,
                    },
                    f,
                )
        return self

    def transform(self, X):
        df = X.copy()
        df = df.merge(self.user_feature, on="user_id", how="left")
        df = df.merge(self.merchant_feature, on="merchant_id", how="left")
        df = df.merge(self.user_merchant_feature, on=["user_id", "merchant_id"], how="left")
        df = df.drop(columns=["user_id", "merchant_id", "activity_log"], errors="ignore")
        print(f"ğŸ”„ ç‰¹å¾åˆå¹¶åæ•°æ®å½¢çŠ¶: {df.shape}")
        return df

    def explode(self, df: pd.DataFrame):
        df = df[df["activity_log"].notnull()]
        df = df.assign(activity_log=df["activity_log"].str.split("#")).explode("activity_log")

        split_columns = ["item_id", "cate_id", "brand_id", "time", "action_type"]
        df[split_columns] = df["activity_log"].str.split(":", expand=True)
        return df

    def create_user_feature(self, df: pd.DataFrame):
        print("ğŸ”„ è®¡ç®—ç”¨æˆ·ç‰¹å¾...")
        # æ¯ä¸ªç”¨æˆ·ä¸åŒactionç±»å‹çš„è¡Œä¸ºå æ¯”
        action_ratio = (
            df.groupby(["user_id", "action_type"])
            .size()
            .div(df.groupby("user_id").size(), level="user_id")
            .unstack(fill_value=0)
            .add_prefix("action_ratio_")
        )
        action_ratio.columns.name = None

        # è®¡ç®—æ¯ä¸ªç”¨æˆ·æ¯ä¸ªæœˆçš„è¡Œä¸ºå æ¯”
        df["month"] = df["time"].str[:2].astype(int)
        time_ratio = (
            df.groupby(["user_id", "month"])
            .size()
            .div(df.groupby("user_id").size(), level="user_id")
            .unstack(fill_value=0)
            .add_prefix("time_ratio_")
        )
        time_ratio.columns.name = None

        # è®¡ç®—æ¯ä¸ªç”¨æˆ·æ¯ä¸ªæœˆä¸åŒactionçš„å æ¯”ã€‚
        time_action_ratio = df.pivot_table(
            index="user_id",
            columns=["month", "action_type"],
            aggfunc="size",
            fill_value=0,
        ).div(df.groupby("user_id").size(), axis=0)
        time_action_ratio.columns = [
            f"time_action_ratio_month_{month}_action_{action}" for month, action in time_action_ratio.columns
        ]

        # äº¤äº’ç»Ÿè®¡ç‰¹å¾
        user_stats = df.groupby("user_id").agg(
            user_item_count=("item_id", "nunique"),
            user_cate_count=("cate_id", "nunique"),
            user_brand_count=("brand_id", "nunique"),
            user_merchant_count=("merchant_id", "nunique"),
            user_action_count=("action_type", "count"),  # æ€»è¡Œä¸ºæ¬¡æ•°
        )

        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        features = (
            action_ratio.join(time_ratio, how="outer")
            .join(time_action_ratio, how="outer")
            .join(user_stats, how="outer")
        )

        self.user_feature = features.reset_index()
        del action_ratio, time_ratio, time_action_ratio, user_stats, features, df
        gc.collect()
        return self

    def create_merchant_feature(self, df: pd.DataFrame):
        print("ğŸ”„ è®¡ç®—å•†æˆ·ç‰¹å¾...")
        # åº—é“ºå„ç§actionçš„å æ¯”
        action_ratio = (
            df.groupby(["merchant_id", "action_type"])
            .size()
            .div(df.groupby("merchant_id").size(), level="merchant_id")
            .unstack(fill_value=0)
            .add_prefix("merchant_action_ratio_")
        )
        action_ratio.columns.name = None

        df["month"] = df["time"].str[:2].astype(int)

        # è®¡ç®—æ¯ä¸ªå•†æˆ·æ¯ä¸ªæœˆçš„è¡Œä¸ºå æ¯”
        time_ratio = (
            df.groupby(["merchant_id", "month"])
            .size()
            .div(df.groupby("merchant_id").size(), level="merchant_id")
            .unstack(fill_value=0)
            .add_prefix("merchant_time_ratio_")
        )
        time_ratio.columns.name = None

        # è®¡ç®—æ¯ä¸ªå•†æˆ·æ¯ä¸ªæœˆä¸åŒactionçš„å æ¯”ã€‚
        time_action_ratio = df.pivot_table(
            index="merchant_id",
            columns=["month", "action_type"],
            aggfunc="size",
            fill_value=0,
        ).div(df.groupby(["merchant_id"]).size(), axis=0)
        time_action_ratio.columns = [
            f"merchant_time_action_ratio_month_{month}_action_{action}" for month, action in time_action_ratio.columns
        ]

        # äº¤äº’ç»Ÿè®¡ç‰¹å¾
        merch_stats = df.groupby("merchant_id").agg(
            merch_item_count=("item_id", "nunique"),
            merch_cate_count=("cate_id", "nunique"),
            merch_brand_count=("brand_id", "nunique"),
            merch_user_count=("user_id", "nunique"),
            merch_action_count=("action_type", "count"),  # æ€»è¡Œä¸ºæ¬¡æ•°
        )
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        self.features = (
            action_ratio.join(time_ratio, how="outer")
            .join(time_action_ratio, how="outer")
            .join(merch_stats, how="outer")
        )

        self.merchant_feature = self.features.reset_index()
        del action_ratio, time_ratio, time_action_ratio, merch_stats, df
        gc.collect()
        return self

    def create_user_merchant_feature(self, df: pd.DataFrame):
        print("ğŸ”„ è®¡ç®—ç”¨æˆ·-å•†æˆ·äº¤äº’ç‰¹å¾...")
        user_merchant_interactions = df.groupby(["user_id", "merchant_id"]).size()
        user_total_interactions = df.groupby("user_id").size()

        user_merchant_ratio = (
            user_merchant_interactions.div(user_total_interactions, level="user_id")
            .reset_index()
            .rename(columns={0: "user_merchant_interaction_ratio"})
        )

        merchant_total_interactions = df.groupby("merchant_id").size()
        merchant_user_ratio = (
            user_merchant_interactions.div(merchant_total_interactions, level="merchant_id")
            .reset_index()
            .rename(columns={0: "merchant_user_interaction_ratio"})
        )

        user_merchant_features = user_merchant_ratio.merge(
            merchant_user_ratio, on=["user_id", "merchant_id"], how="outer"
        )

        self.user_merchant_feature = user_merchant_features.reset_index()
        del (
            user_merchant_ratio,
            merchant_user_ratio,
            user_merchant_interactions,
            user_total_interactions,
            merchant_total_interactions,
            df,
        )
        gc.collect()
        return self


class ModelConfig(BaseModel):
    model_type: str = "rf"  # å¯é€‰ 'rf', 'xgb', 'lgb'
    n_estimators: int = 200
    max_depth: int = 10
    learning_rate: float = 0.1
    scale_pos_weight: float = 7.0  # ç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡


class TCDataConfig(BaseModel):
    fill_median_cols: list[str] = []
    fill_mode_cols: list[str] = []

    cache_clean_result: bool = False
    cache_clean_path: str = "data/cleaned_data.pkl"

    model: ModelConfig = ModelConfig()


class UserMerchantFeatureTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.user_merchant_features = None
        return

    def fit(self, X, y=None):
        X_copy = X.copy()

        user_merchant_interactions = X_copy.groupby(["user_id", "merchant_id"]).size()
        user_total_interactions = X_copy.groupby("user_id").size()

        user_merchant_ratio = (
            user_merchant_interactions.div(user_total_interactions, level="user_id")
            .reset_index()
            .rename(columns={0: "user_merchant_interaction_ratio"})
        )

        merchant_total_interactions = X_copy.groupby("merchant_id").size()
        merchant_user_ratio = (
            user_merchant_interactions.div(merchant_total_interactions, level="merchant_id")
            .reset_index()
            .rename(columns={0: "merchant_user_interaction_ratio"})
        )

        user_merchant_features = user_merchant_ratio.merge(
            merchant_user_ratio, on=["user_id", "merchant_id"], how="outer"
        )

        self.user_merchant_features = user_merchant_features.reset_index()
        del (
            user_merchant_ratio,
            merchant_user_ratio,
            user_merchant_interactions,
            user_total_interactions,
            merchant_total_interactions,
            X_copy,
        )
        gc.collect()
        return self

    def transform(self, X):
        """âœ… å®ç°ç‰¹å¾åˆå¹¶é€»è¾‘"""
        if self.user_merchant_features is None:
            raise ValueError("Transformer has not been fitted yet.")

        X_transformed = X.copy()

        # åˆå¹¶ç”¨æˆ·-å•†æˆ·äº¤äº’ç‰¹å¾
        if all(col in X_transformed.columns for col in ["user_id", "merchant_id"]):
            X_transformed = X_transformed.merge(self.user_merchant_features, on=["user_id", "merchant_id"], how="left")
        else:
            print("âš ï¸ è¾“å…¥æ•°æ®ç¼ºå°‘ user_id æˆ– merchant_id åˆ—ï¼Œè·³è¿‡ç”¨æˆ·-å•†æˆ·ç‰¹å¾åˆå¹¶")

        return X_transformed


class UserFeatureTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.features = None
        return

    def fit(self, X, y=None):
        X_copy = X.copy()
        X_copy = X_copy[X_copy["activity_log"].notnull()]
        X_copy = X_copy.assign(activity_log=X_copy["activity_log"].str.split("#")).explode("activity_log")

        X_copy[["item_id", "cate_id", "brand_id", "time", "action_type"]] = X_copy["activity_log"].str.split(
            ":", expand=True
        )

        # æ¯ä¸ªç”¨æˆ·ä¸åŒactionç±»å‹çš„è¡Œä¸ºå æ¯”
        action_ratio = (
            X_copy.groupby(["user_id", "action_type"])
            .size()
            .div(X_copy.groupby("user_id").size(), level="user_id")
            .unstack(fill_value=0)
            .add_prefix("action_ratio_")
        )
        action_ratio.columns.name = None

        # è®¡ç®—æ¯ä¸ªç”¨æˆ·æ¯ä¸ªæœˆçš„è¡Œä¸ºå æ¯”
        X_copy["month"] = X_copy["time"].str[:2].astype(int)
        time_ratio = (
            X_copy.groupby(["user_id", "month"])
            .size()
            .div(X_copy.groupby("user_id").size(), level="user_id")
            .unstack(fill_value=0)
            .add_prefix("time_ratio_")
        )
        time_ratio.columns.name = None

        # è®¡ç®—æ¯ä¸ªç”¨æˆ·æ¯ä¸ªæœˆä¸åŒactionçš„å æ¯”ã€‚
        time_action_ratio = X_copy.pivot_table(
            index="user_id",
            columns=["month", "action_type"],
            aggfunc="size",
            fill_value=0,
        ).div(X_copy.groupby("user_id").size(), axis=0)
        time_action_ratio.columns = [
            f"time_action_ratio_month_{month}_action_{action}" for month, action in time_action_ratio.columns
        ]

        # äº¤äº’ç»Ÿè®¡ç‰¹å¾
        user_stats = X_copy.groupby("user_id").agg(
            user_item_count=("item_id", "nunique"),
            user_cate_count=("cate_id", "nunique"),
            user_brand_count=("brand_id", "nunique"),
            user_merchant_count=("merchant_id", "nunique"),
            user_action_count=("action_type", "count"),  # æ€»è¡Œä¸ºæ¬¡æ•°
        )

        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        features = (
            action_ratio.join(time_ratio, how="outer")
            .join(time_action_ratio, how="outer")
            .join(user_stats, how="outer")
        )

        self.features = features.reset_index()
        del action_ratio, time_ratio, time_action_ratio, user_stats, features, X_copy
        gc.collect()
        return self

    def transform(self, X):
        # å‡è®¾ X æ˜¯ä¸€ä¸ªåŒ…å«ç”¨æˆ·ä¿¡æ¯çš„ DataFrame
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„ç‰¹å¾å·¥ç¨‹æ­¥éª¤
        if self.features is None:
            raise ValueError("The transformer has not been fitted yet.")
        X_transformed = X.copy()
        X_transformed = X_transformed.merge(self.features, how="left", on="user_id")
        return X_transformed


class MerchantFeatureTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.features = None
        return

    def fit(self, X, y=None):
        X_copy = X.copy()
        X_copy = X_copy[X_copy["activity_log"].notnull()]
        X_copy = X_copy.assign(activity_log=X_copy["activity_log"].str.split("#")).explode("activity_log")

        X_copy[["item_id", "cate_id", "brand_id", "time", "action_type"]] = X_copy["activity_log"].str.split(
            ":", expand=True
        )

        # åº—é“ºå„ç§actionçš„å æ¯”
        action_ratio = (
            X_copy.groupby(["merchant_id", "action_type"])
            .size()
            .div(X_copy.groupby("merchant_id").size(), level="merchant_id")
            .unstack(fill_value=0)
            .add_prefix("merchant_action_ratio_")
        )
        action_ratio.columns.name = None

        X_copy["month"] = X_copy["time"].str[:2].astype(int)

        # è®¡ç®—æ¯ä¸ªå•†æˆ·æ¯ä¸ªæœˆçš„è¡Œä¸ºå æ¯”
        time_ratio = (
            X_copy.groupby(["merchant_id", "month"])
            .size()
            .div(X_copy.groupby("merchant_id").size(), level="merchant_id")
            .unstack(fill_value=0)
            .add_prefix("merchant_time_ratio_")
        )
        time_ratio.columns.name = None

        # è®¡ç®—æ¯ä¸ªå•†æˆ·æ¯ä¸ªæœˆä¸åŒactionçš„å æ¯”ã€‚
        time_action_ratio = X_copy.pivot_table(
            index="merchant_id",
            columns=["month", "action_type"],
            aggfunc="size",
            fill_value=0,
        ).div(X_copy.groupby(["merchant_id"]).size(), axis=0)
        time_action_ratio.columns = [
            f"merchant_time_action_ratio_month_{month}_action_{action}" for month, action in time_action_ratio.columns
        ]

        # äº¤äº’ç»Ÿè®¡ç‰¹å¾
        merch_stats = X_copy.groupby("merchant_id").agg(
            merch_item_count=("item_id", "nunique"),
            merch_cate_count=("cate_id", "nunique"),
            merch_brand_count=("brand_id", "nunique"),
            merch_user_count=("user_id", "nunique"),
            merch_action_count=("action_type", "count"),  # æ€»è¡Œä¸ºæ¬¡æ•°
        )
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        self.features = (
            action_ratio.join(time_ratio, how="outer")
            .join(time_action_ratio, how="outer")
            .join(merch_stats, how="outer")
        )

        self.features = self.features.reset_index()
        del action_ratio, time_ratio, time_action_ratio, merch_stats, X_copy
        gc.collect()

        return self

    def transform(self, X):
        X_transformed = X.copy()
        X_transformed = X_transformed.merge(self.features, how="left", on="merchant_id")
        return X_transformed


def create_clean_pipeline(conf: TCDataConfig) -> Pipeline:
    """Create a data cleaning pipeline."""
    column_transformer = ColumnTransformer(
        [
            ("median_fill", SimpleImputer(strategy="median"), conf.fill_median_cols),
            ("mode_fill", SimpleImputer(strategy="most_frequent"), conf.fill_mode_cols),
        ],
        remainder="passthrough",
        verbose_feature_names_out=False,
        verbose=True,
    )
    column_transformer.set_output(transform="pandas")

    steps = [
        ("column_transformer", column_transformer),
        ("user_feat", UserFeatureTransformer()),
        ("merch_feat", MerchantFeatureTransformer()),
        ("user_merchant_feat", UserMerchantFeatureTransformer()),
    ]
    return Pipeline(steps, verbose=True)


def create_sample_pipeline(conf: TCDataConfig):
    """åˆ›å»ºæ•°æ®é‡‡æ ·ç®¡é“"""
    try:
        steps = [("smote", SMOTE(random_state=42, k_neighbors=5))]

        return ImbPipeline(steps)
    except ImportError:
        print("âš ï¸ imbalanced-learn æœªå®‰è£…ï¼Œè·³è¿‡é‡‡æ ·ç®¡é“")
        return None


def create_model_pipeline(conf: ModelConfig) -> Pipeline:
    """åˆ›å»ºæ¨¡å‹è®­ç»ƒç®¡é“ï¼Œæ”¯æŒå¤šç§æ¨¡å‹"""
    steps = [("scaler", StandardScaler())]

    if conf.model_type == "rf":
        classifier = RandomForestClassifier(
            n_estimators=conf.n_estimators,
            max_depth=conf.max_depth,
            random_state=42,
            n_jobs=-1,
            class_weight="balanced",
        )
    elif conf.model_type == "xgb":
        if xgb is None:
            raise ImportError("XGBoost æœªå®‰è£…ï¼Œè¯·å®‰è£…åä½¿ç”¨: pip install xgboost")
        classifier = xgb.XGBClassifier(
            n_estimators=conf.n_estimators,
            max_depth=conf.max_depth,
            random_state=42,
            n_jobs=-1,
            scale_pos_weight=conf.scale_pos_weight,
        )
    elif conf.model_type == "lgb":
        if lgb is None:
            raise ImportError("LightGBM æœªå®‰è£…ï¼Œè¯·å®‰è£…åä½¿ç”¨: pip install lightgbm")
        classifier = lgb.LGBMClassifier(
            n_estimators=conf.n_estimators,
            max_depth=conf.max_depth,
            random_state=42,
            n_jobs=-1,
            class_weight="balanced",
        )
    else:
        raise ValueError("model_type å¿…é¡»æ˜¯ 'rf', 'xgb' æˆ– 'lgb'")

    steps.append(("classifier", classifier))
    return Pipeline(steps)


class TCDataPipeline:
    def __init__(self, conf: TCDataConfig):
        self.conf = conf
        self.cache_feature_transformer = CacheFeatureTransformer(
            enable_cache=conf.cache_clean_result,
            cache_path=conf.cache_clean_path,
        )
        self.clean_pipe = create_clean_pipeline(conf)
        self.sample_pipe = None  # create_sample_pipeline(conf)
        self.model_pipe = create_model_pipeline(conf.model)

        # å­˜å‚¨æ•°æ®é›†
        self.X_train = None
        self.X_val = None
        self.X_test = None
        self.y_train = None
        self.y_val = None
        self.y_test = None

        # å­˜å‚¨æ¸…æ´—åçš„å®Œæ•´æ•°æ®
        self.X_clean = None
        self.y_clean = None

        # è®­ç»ƒçŠ¶æ€
        self.is_fitted = False

    def _clean(self, X, y=None):
        # 1. æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹
        if self.conf.cache_clean_result:
            if os.path.exists(self.conf.cache_clean_path):
                print(f"â™»ï¸ ä»ç¼“å­˜åŠ è½½æ¸…æ´—åçš„æ•°æ®: {self.conf.cache_clean_path}")
                with open(self.conf.cache_clean_path, "rb") as f:
                    d = pickle.load(f)
                # cleaned_data = pd.read_pickle(self.conf.cache_clean_path)
                X_clean, y_clean = d["X"], d["y"]
            else:
                X_clean = self.clean_pipe.fit_transform(X, y)
                y_clean = y.copy()
                cleaned_data = {"X": X_clean, "y": y_clean}
                with open(self.conf.cache_clean_path, "wb") as f:
                    pickle.dump(cleaned_data, f)
                print(f"âœ… æ¸…æ´—åçš„æ•°æ®å·²ç¼“å­˜åˆ°: {self.conf.cache_clean_path}")
        else:
            X_clean = self.clean_pipe.fit_transform(X, y)
            y_clean = y.copy()

        self.X_clean = X_clean
        self.y_clean = y_clean

    # def split_dataset(self, X, y, val_size=0.2, random_state=42):
    #     train_data_mask = y.isin([0, 1])
    #     test_data_mask = y.isnull()

    #     train_X, train_y = X[train_data_mask], y[train_data_mask]
    #     test_X, test_y = X[test_data_mask], y[test_data_mask]

    #     assert len(train_X) == len(train_y), "è®­ç»ƒé›†ç‰¹å¾å’Œæ ‡ç­¾æ•°é‡ä¸åŒ¹é…"
    #     assert train_y.nunique() > 1, "è®­ç»ƒé›†æ ‡ç­¾å¿…é¡»è‡³å°‘åŒ…å«ä¸¤ä¸ªç±»åˆ«"
    #     assert test_y.isnull().all(), "æµ‹è¯•é›†æ ‡ç­¾å¿…é¡»å…¨éƒ¨ä¸ºç©º"
    #     print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_X.shape}, æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_X.shape}")

    #     self.cache_feature_transformer.fit(X, y)
    #     train_X = self.cache_feature_transformer.transform(train_X)

    #     print("\nğŸ”„ Step 2: æ•°æ®é›†æ‹†åˆ†...")
    #     train_X, val_X, train_y, val_y = train_test_split(
    #         train_X, train_y, test_size=val_size, random_state=random_state, stratify=train_y
    #     )
    #     return train_X, train_y, val_X, val_y, test_X, test_y

    def preprocess(self, X: pd.DataFrame, y: pd.Series = None):
        """
        ä»…è¿›è¡Œæ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹
        """
        self.cache_feature_transformer.fit(X, y)

        train_mask = y.isin([0, 1])
        test_mask = y.isnull()

        X_train, y_train = X[train_mask], y[train_mask]
        X_test, y_test = X[test_mask], y[test_mask]

        X_train = self.cache_feature_transformer.transform(X_train)
        X_test = self.cache_feature_transformer.transform(X_test)

        return X_train, y_train, X_test, y_test

    def fit(self, X, y):
        """
        å®Œæ•´çš„è®­ç»ƒæµç¨‹
        """
        train_X, train_y, test_X, test_y = self.preprocess(X, y)
        self.summary(train_X, None, test_X)
        # print("ğŸ”„ Step 1: æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹...")
        # self._clean(X, y)
        # self.X_clean = self.X_clean.drop(columns=["user_id", "merchant_id", "activity_log"], errors="ignore")
        # print(f"âœ… æ¸…æ´—åæ•°æ®å½¢çŠ¶: {self.X_clean.shape}")
        # print(f"âœ… ç‰¹å¾æ•°é‡: {self.X_clean.shape[1]}")

        # 2. æ•°æ®é›†æ‹†åˆ†
        # self.split_dataset(val_size, random_state)
        # self.X_train = self.X_train.drop(columns=["user_id", "merchant_id", "activity_log"], errors="ignore")
        # 3. æ•°æ®é‡‡æ · (å¦‚æœæœ‰è®­ç»ƒæ•°æ®ä¸”é…ç½®äº†é‡‡æ ·ç®¡é“)
        if self.sample_pipe is not None and train_X is not None:
            print("\nğŸ”„ Step 3: æ•°æ®é‡‡æ ·...")
            try:
                train_X, train_y = self.sample_pipe.fit_resample(train_X, train_y)
                print(f"âœ… é‡‡æ ·åè®­ç»ƒé›†å½¢çŠ¶: {train_X.shape}")
                print(f"âœ… é‡‡æ ·åæ ‡ç­¾åˆ†å¸ƒ:\n{pd.Series(train_y).value_counts()}")
            except Exception as e:
                print(f"âš ï¸ é‡‡æ ·å¤±è´¥ï¼Œè·³è¿‡é‡‡æ ·æ­¥éª¤: {e}")

        # 4. æ¨¡å‹è®­ç»ƒ (å¦‚æœæœ‰è®­ç»ƒæ•°æ®ä¸”é…ç½®äº†æ¨¡å‹ç®¡é“)
        if self.model_pipe is not None and train_X is not None and train_y is not None:
            print("\nğŸ”„ Step 4: æ¨¡å‹è®­ç»ƒ...")
            try:
                self.feature_names = train_X.columns.tolist()
                self.model_pipe.fit(train_X, train_y)
                print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
                self.is_fitted = True
            except Exception as e:
                print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
                self.is_fitted = False
        return self

    # def split_dataset(self, val_size=0.2, random_state=42):
    #     """
    #     æ‹†åˆ†æ•°æ®é›†ï¼Œæ”¯æŒæœ‰æµ‹è¯•é›†æ ‡ç­¾ä¸ºç©ºçš„æƒ…å†µ
    #     """
    #     X_clean = self.X_clean.reset_index(drop=True)
    #     y_clean = self.y_clean.reset_index(drop=True)

    #     print("æ•°æ®é›†å¤§å°ï¼š ", X_clean.shape, y_clean.shape)
    #     # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºæ ‡ç­¾ï¼ˆæµ‹è¯•é›†ï¼‰
    #     if y_clean.isnull().any():
    #         print("ğŸ“‹ æ£€æµ‹åˆ°ç©ºæ ‡ç­¾ï¼Œå°†å…¶ä½œä¸ºæµ‹è¯•é›†...")
    #         test_mask = y_clean.isnull()
    #         self.X_test = X_clean[test_mask].reset_index(drop=True)
    #         self.y_test = y_clean[test_mask].reset_index(drop=True)

    #         # å‰©ä½™çš„ä½œä¸ºè®­ç»ƒ+éªŒè¯é›†
    #         train_val_mask = y_clean.isin([1, 0])
    #         X_train_val = X_clean[train_val_mask].reset_index(drop=True)
    #         y_train_val = y_clean[train_val_mask].reset_index(drop=True)
    #     else:
    #         print("ğŸ“‹ æœªæ£€æµ‹åˆ°ç©ºæ ‡ç­¾ï¼Œä»å®Œæ•´æ•°æ®ä¸­æ‹†åˆ†æµ‹è¯•é›†...")
    #         # å¦‚æœæ²¡æœ‰ç©ºæ ‡ç­¾ï¼Œåˆ™éšæœºæ‹†åˆ†æµ‹è¯•é›†ï¼ˆ20%ï¼‰
    #         X_train_val, self.X_test, y_train_val, self.y_test = train_test_split(
    #             X_clean,
    #             y_clean,
    #             test_size=0.2,
    #             random_state=random_state,
    #             stratify=y_clean if y_clean.nunique() > 1 else None,
    #         )

    #     # ä»è®­ç»ƒ+éªŒè¯é›†ä¸­æ‹†åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    #     if val_size > 0 and len(X_train_val) > 1:
    #         try:
    #             self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(
    #                 X_train_val,
    #                 y_train_val,
    #                 test_size=val_size,
    #                 random_state=random_state,
    #                 stratify=y_train_val if y_train_val.nunique() > 1 else None,
    #             )
    #         except ValueError as e:
    #             print(f"âš ï¸ åˆ†å±‚æ‹†åˆ†å¤±è´¥ï¼Œä½¿ç”¨éšæœºæ‹†åˆ†: {e}")
    #             self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(
    #                 X_train_val,
    #                 y_train_val,
    #                 test_size=val_size,
    #                 random_state=random_state,
    #             )
    #     else:
    #         # å¦‚æœä¸éœ€è¦éªŒè¯é›†æˆ–æ•°æ®å¤ªå°‘
    #         self.X_train = X_train_val
    #         self.y_train = y_train_val
    #         self.X_val = None
    #         self.y_val = None

    #     # æ‰“å°æ‹†åˆ†ç»“æœ
    #     print(f"âœ… è®­ç»ƒé›†: {self.X_train.shape if self.X_train is not None else 'None'}")
    #     print(f"âœ… éªŒè¯é›†: {self.X_val.shape if self.X_val is not None else 'None'}")
    #     print(f"âœ… æµ‹è¯•é›†: {self.X_test.shape if self.X_test is not None else 'None'}")

    #     # æ‰“å°æ ‡ç­¾åˆ†å¸ƒ
    #     if self.y_train is not None:
    #         print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:\n{pd.Series(self.y_train).value_counts()}")
    #     if self.y_val is not None:
    #         print(f"éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ:\n{pd.Series(self.y_val).value_counts()}")

    def transform(self, X):
        """å¯¹æ–°æ•°æ®è¿›è¡Œé¢„å¤„ç†"""
        if self.clean_pipe is None:
            raise ValueError("Pipeline has not been fitted yet.")

        return self.clean_pipe.transform(X)

    def predict(self, X):
        """é¢„æµ‹æ–°æ•°æ®"""
        if not self.is_fitted:
            raise ValueError("Model has not been trained yet.")

        # X_processed = self.transform(X)
        return self.model_pipe.predict(X[self.feature_names])

    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if not self.is_fitted:
            raise ValueError("Model has not been trained yet.")

        # X_processed = self.transform(X)
        return self.model_pipe.predict_proba(X[self.feature_names])

    def evaluate(self, val_X, val_y):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        if not self.is_fitted:
            raise ValueError("Model has not been trained yet.")

        if val_X is None or val_y is None:
            print("âŒ éªŒè¯é›†æ•°æ®ä¸å­˜åœ¨")
            return None

        # è¿‡æ»¤æ‰ç©ºæ ‡ç­¾
        mask = val_y.notna()
        if not mask.any():
            print("âŒ éªŒè¯é›†æ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾")
            return None

        X_eval_clean = val_X[mask]
        y_eval_clean = val_y[mask]

        # é¢„æµ‹
        y_pred = self.model_pipe.predict(X_eval_clean[self.feature_names])

        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(y_eval_clean, y_pred)

        print("ğŸ“Š éªŒè¯é›†è¯„ä¼°ç»“æœ:")
        print(f"å‡†ç¡®ç‡: {accuracy:.4f}")

        # å¦‚æœæ˜¯äºŒåˆ†ç±»ï¼Œè®¡ç®—AUC
        if hasattr(self.model_pipe, "predict_proba") and len(np.unique(y_eval_clean)) == 2:
            try:
                y_pred_proba = self.model_pipe.predict_proba(X_eval_clean[self.feature_names])
                auc = roc_auc_score(y_eval_clean, y_pred_proba[:, 1])
                print(f"AUC: {auc:.4f}")
            except Exception as e:
                print(f"âš ï¸ AUCè®¡ç®—å¤±è´¥: {e}")
                auc = None
        else:
            auc = None

        print(f"\nè¯¦ç»†æŠ¥å‘Š:\n{classification_report(y_eval_clean, y_pred)}")

        return {
            "accuracy": accuracy,
            "auc": auc,
            "y_true": y_eval_clean,
            "y_pred": y_pred,
        }

    def get_feature_importance(self, top_n=20):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if not self.is_fitted:
            print("æ¨¡å‹å°šæœªè®­ç»ƒ")
            return None

        # å°è¯•è·å–ç‰¹å¾é‡è¦æ€§
        model = self.model_pipe
        if hasattr(model, "named_steps"):
            # å¦‚æœæ˜¯Pipelineï¼Œè·å–æœ€åä¸€ä¸ªæ­¥éª¤
            final_step = list(model.named_steps.values())[-1]
        else:
            final_step = model

        if hasattr(final_step, "feature_importances_"):
            importances = final_step.feature_importances_
            feature_names = self.X_train.columns

            # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
            importance_df = pd.DataFrame({"feature": feature_names, "importance": importances}).sort_values(
                "importance", ascending=False
            )

            print(f"ğŸ” Top {top_n} é‡è¦ç‰¹å¾:")
            print(importance_df.head(top_n))

            return importance_df
        else:
            print("æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
            return None

    def summary(self, train_X, val_X, test_X):
        """æ‰“å°ç®¡é“æ‘˜è¦ä¿¡æ¯"""
        print("ğŸ“‹ TCDataPipeline æ‘˜è¦:")
        print(f"æ•°æ®æ¸…æ´—ç®¡é“: {self.clean_pipe is not None}")
        print(f"é‡‡æ ·ç®¡é“: {self.sample_pipe is not None}")
        print(f"æ¨¡å‹ç®¡é“: {self.model_pipe is not None}")
        print(f"æ¨¡å‹å·²è®­ç»ƒ: {self.is_fitted}")

        if train_X is not None:
            print(f"è®­ç»ƒé›†: {train_X.shape}")
        if val_X is not None:
            print(f"éªŒè¯é›†: {val_X.shape}")
        if test_X is not None:
            print(f"æµ‹è¯•é›†: {test_X.shape}")

    def tune_model(self, X, y, param_grid=None, search_type="grid", cv=3, scoring="roc_auc", n_iter=20):
        """
        è‡ªåŠ¨è°ƒå‚ï¼Œæ”¯æŒ RF/XGB/LGB
        param_grid: dictï¼Œå‚æ•°æœç´¢ç©ºé—´
        search_type: "grid" æˆ– "random"
        """

        train_X, train_y, _, _ = self.preprocess(X, y)
        if train_X is None or train_y is None:
            print("âŒ è®­ç»ƒé›†ä¸å­˜åœ¨ï¼Œæ— æ³•è°ƒå‚")
            return None

        if param_grid is None:
            # é»˜è®¤å‚æ•°ç©ºé—´
            if self.conf.model.model_type == "rf":
                param_grid = {
                    "classifier__n_estimators": [100, 200, 500],
                    "classifier__max_depth": [4, 6, 10, 16],
                }
            elif self.conf.model.model_type == "xgb":
                param_grid = {
                    "classifier__n_estimators": [200, 500, 1000],
                    "classifier__max_depth": [4, 6, 10],
                    "classifier__learning_rate": [0.05, 0.1, 0.2],
                    "classifier__scale_pos_weight": [5, 7, 10],
                }
            elif self.conf.model.model_type == "lgb":
                param_grid = {
                    "classifier__n_estimators": [200, 500, 1000],
                    "classifier__max_depth": [4, 6, 10],
                    "classifier__learning_rate": [0.05, 0.1, 0.2],
                    "classifier__class_weight": ["balanced", None],
                }

        search_cls = GridSearchCV if search_type == "grid" else RandomizedSearchCV

        if search_type == "grid":
            search = search_cls(
                self.model_pipe,
                param_grid,
                cv=cv,
                scoring=scoring,
                verbose=2,
                n_jobs=-1,
            )
        else:
            search = search_cls(
                self.model_pipe,
                param_grid,
                cv=cv,
                scoring=scoring,
                n_iter=n_iter,
                verbose=2,
                n_jobs=-1,
            )
        print("ğŸ” å¼€å§‹æ¨¡å‹è°ƒå‚...")
        search.fit(train_X, train_y)
        print(f"âœ… æœ€ä¼˜å‚æ•°: {search.best_params_}")
        print(f"âœ… æœ€ä¼˜åˆ†æ•°: {search.best_score_:.4f}")

        self.model_pipe = search.best_estimator_
        self.is_fitted = True
        return search

    def export_prediction(self, X: pd.DataFrame, filename="prediction.csv"):
        """
        å¯¼å‡ºé¢„æµ‹ç»“æœåˆ° prediction.csv
        æ ¼å¼ï¼šuser_id, merchant_id, prob
        """
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œæ— æ³•å¯¼å‡ºé¢„æµ‹ç»“æœã€‚")

        _, _, X_test, _ = self.preprocess(X)

        # åªä¿ç•™è®­ç»ƒç”¨çš„ç‰¹å¾
        drop_cols = ["user_id", "merchant_id", "activity_log"]
        X_pred = X_test.drop(columns=[col for col in drop_cols if col in X_test.columns], errors="ignore")

        # é¢„æµ‹æ¦‚ç‡
        prob = self.predict_proba(X_pred)[:, 1]  # å–æ­£ç±»æ¦‚ç‡

        # æ„å»ºç»“æœ DataFrame
        result = pd.DataFrame(
            {"user_id": X_test["user_id"].values, "merchant_id": X_test["merchant_id"].values, "prob": prob}
        )

        # ä¿å­˜ä¸º CSV
        result.to_csv(filename, index=False)
        print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° {filename}")


def analysis():
    """Perform data analysis."""
    dataset = load_data()

    print("train f1 info:")
    print(dataset.train_f1.info())
    print("=" * 50)

    print("user f1 info: ")
    print(dataset.user_f1.info())
    print("=" * 50)

    print("log f1 info: ")
    print(dataset.log_f1.info())
    print("=" * 50)

    print("train f2 info: ")
    print(dataset.train_f2.info())
    print("=" * 50)

    print("test f2 info:")
    print(dataset.test_f2.info())
    print("=" * 50)


def run():
    df = load_dataframe()
    X, y = df.drop(columns=["label"]), df["label"]
    pipe = create_clean_pipeline(
        TCDataConfig(
            fill_median_cols=["age_range"],
            fill_mode_cols=["gender"],
        )
    )
    pipe = TCDataPipeline(
        TCDataConfig(
            fill_median_cols=["age_range"],
            fill_mode_cols=["gender"],
            cache_clean_result=True,
            cache_clean_path="../data/cleaned_data.pkl",
            model=ModelConfig(model_type="lgb", n_estimators=500, max_depth=4, scale_pos_weight=7.0),
        )
    )

    pipe.fit(X, y)
    pipe.tune_model(X, y)
    # pipe.evaluate(stage="val")
    # print(X.head(10))

    if pipe.X_test is not None:
        pipe.export_prediction(X, filename="../output/prediction.csv")


if __name__ == "__main__":
    run()
