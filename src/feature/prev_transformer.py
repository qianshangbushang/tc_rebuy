import os
import pickle
from collections import defaultdict

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin


class PrevTransformer(BaseEstimator, TransformerMixin):
    """å…ˆéªŒæ¦‚ç‡ç‰¹å¾è½¬æ¢å™¨ï¼Œç”Ÿæˆç”¨æˆ·å¤è´­å’Œå•†æˆ·è¢«å¤è´­çš„å…ˆéªŒç‰¹å¾"""

    def __init__(
        self,
        min_user_interactions: int = 3,
        min_merchant_interactions: int = 5,
        time_window_days: int = 365,
        repurchase_window_days: int = 90,
        enable_cache: bool = True,
        cache_path: str = "../output/prev_feature.pkl",
        pseudo_count: float = 1.0,
    ):
        """
        Args:
            min_user_interactions: ç”¨æˆ·æœ€å°äº¤äº’æ¬¡æ•°é˜ˆå€¼
            min_merchant_interactions: å•†æˆ·æœ€å°äº¤äº’æ¬¡æ•°é˜ˆå€¼
            time_window_days: è§‚å¯Ÿæ—¶é—´çª—å£ï¼ˆå¤©ï¼‰
            repurchase_window_days: å¤è´­æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            cache_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
            pseudo_count: è´å¶æ–¯å¹³æ»‘ä¼ªè®¡æ•°
        """
        self.min_user_interactions = min_user_interactions
        self.min_merchant_interactions = min_merchant_interactions
        self.time_window_days = time_window_days
        self.repurchase_window_days = repurchase_window_days
        self.enable_cache = enable_cache
        self.cache_path = cache_path
        self.pseudo_count = pseudo_count

        # å…ˆéªŒæ¦‚ç‡ç»Ÿè®¡
        self.user_prior_stats = {}  # ç”¨æˆ·å¤è´­å…ˆéªŒç»Ÿè®¡
        self.merchant_prior_stats = {}  # å•†æˆ·è¢«å¤è´­å…ˆéªŒç»Ÿè®¡
        self.global_stats = {}  # å…¨å±€ç»Ÿè®¡

        # æœ€ç»ˆç‰¹å¾
        self.user_prior_features = None  # ç”¨æˆ·å…ˆéªŒç‰¹å¾DataFrame
        self.merchant_prior_features = None  # å•†æˆ·å…ˆéªŒç‰¹å¾DataFrame

        self.is_fitted = False

    def fit(self, X, y=None):
        """è®­ç»ƒå…ˆéªŒæ¦‚ç‡ç‰¹å¾"""
        if self.enable_cache and os.path.exists(self.cache_path):
            print(f"â™»ï¸ ä»ç¼“å­˜åŠ è½½å…ˆéªŒç‰¹å¾: {self.cache_path}")
            with open(self.cache_path, "rb") as f:
                cached_data = pickle.load(f)

            self.user_prior_features = cached_data["user_prior_features"]
            self.merchant_prior_features = cached_data["merchant_prior_features"]
            self.global_stats = cached_data["global_stats"]

            print("âœ… å…ˆéªŒç‰¹å¾åŠ è½½å®Œæˆ")
            self.is_fitted = True
            return self

        print("ğŸ”„ å¼€å§‹è®­ç»ƒå…ˆéªŒæ¦‚ç‡ç‰¹å¾...")

        # 1. å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆåªä½¿ç”¨æœ‰æ ‡ç­¾çš„æ•°æ®ï¼‰
        if y is not None:
            train_mask = y.isin([0, 1])
            X_train = X[train_mask].copy()
            y_train = y[train_mask].copy()
            print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(X_train)} æ ·æœ¬")
        else:
            X_train = X.copy()
            y_train = None
            print("âš ï¸ æœªæä¾›æ ‡ç­¾ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®")

        # 2. è§£ææ´»åŠ¨æ—¥å¿—å¹¶æ„å»ºç”¨æˆ·-å•†æˆ·äº¤äº’æ•°æ®
        interaction_data = self._parse_interactions(X_train)
        print(f"ğŸ“Š è§£æå‡º {len(interaction_data)} æ¡äº¤äº’è®°å½•")

        # 3. è®¡ç®—å…¨å±€ç»Ÿè®¡
        self._calculate_global_stats(interaction_data, y_train)

        # 4. è®¡ç®—ç”¨æˆ·å…ˆéªŒæ¦‚ç‡
        self._calculate_user_priors(interaction_data, y_train)

        # 5. è®¡ç®—å•†æˆ·å…ˆéªŒæ¦‚ç‡
        self._calculate_merchant_priors(interaction_data, y_train)

        # 6. ç”Ÿæˆç‰¹å¾DataFrame
        self._generate_feature_dataframes(X_train)

        # 7. ç¼“å­˜ç»“æœ
        self._cache_results()

        self.is_fitted = True
        print("âœ… å…ˆéªŒæ¦‚ç‡ç‰¹å¾è®­ç»ƒå®Œæˆ")
        return self

    def transform(self, X):
        """è½¬æ¢æ•°æ®ä¸ºå…ˆéªŒæ¦‚ç‡ç‰¹å¾"""
        if not self.is_fitted:
            raise ValueError("PrevTransformer has not been fitted yet.")

        print("ğŸ”„ å¼€å§‹å…ˆéªŒæ¦‚ç‡ç‰¹å¾è½¬æ¢...")

        result = X.copy()

        # å…³è”ç”¨æˆ·å…ˆéªŒç‰¹å¾
        if self.user_prior_features is not None:
            result = result.merge(self.user_prior_features, on="user_id", how="left")
            user_features = len([col for col in self.user_prior_features.columns if col != "user_id"])
            print(f"  âœ… å…³è”ç”¨æˆ·å…ˆéªŒç‰¹å¾: {user_features} ç»´")

        # å…³è”å•†æˆ·å…ˆéªŒç‰¹å¾
        if self.merchant_prior_features is not None:
            result = result.merge(self.merchant_prior_features, on="merchant_id", how="left")
            merchant_features = len([col for col in self.merchant_prior_features.columns if col != "merchant_id"])
            print(f"  âœ… å…³è”å•†æˆ·å…ˆéªŒç‰¹å¾: {merchant_features} ç»´")

        # å¡«å……ç¼ºå¤±å€¼
        prev_columns = [col for col in result.columns if col.startswith(("user_prev_", "merchant_prev_"))]
        for col in prev_columns:
            if col.startswith("user_prev_"):
                result[col] = result[col].fillna(self.global_stats.get("global_user_repurchase_rate", 0.3))
            elif col.startswith("merchant_prev_"):
                result[col] = result[col].fillna(self.global_stats.get("global_merchant_repurchase_rate", 0.3))

        print(f"âœ… å…ˆéªŒæ¦‚ç‡ç‰¹å¾è½¬æ¢å®Œæˆï¼Œæ–°å¢ç»´åº¦: {result.shape[1] - X.shape[1]}")
        return result

    def _parse_interactions(self, X):
        """è§£æactivity_logï¼Œæå–ç”¨æˆ·-å•†æˆ·äº¤äº’æ•°æ®"""
        print("ğŸ“Š è§£æç”¨æˆ·-å•†æˆ·äº¤äº’æ•°æ®...")

        interactions = []

        for idx, row in X.iterrows():
            user_id = row["user_id"]
            merchant_id = row["merchant_id"]
            activity_log = row["activity_log"]

            if pd.isna(activity_log):
                continue

            # è§£ææ´»åŠ¨æ—¥å¿—
            activities = self._parse_activity_log(activity_log)

            # æå–è´­ä¹°è¡Œä¸º
            purchases = [act for act in activities if act.get("action_type") == "2"]

            if purchases:
                interactions.append(
                    {
                        "user_id": user_id,
                        "merchant_id": merchant_id,
                        "purchases": purchases,
                        "total_activities": len(activities),
                        "purchase_count": len(purchases),
                        "first_purchase": min(purchases, key=lambda x: x["timestamp"])["timestamp"],
                        "last_purchase": max(purchases, key=lambda x: x["timestamp"])["timestamp"],
                    }
                )

        return interactions

    def _parse_activity_log(self, activity_log):
        """è§£æå•æ¡activity_log"""
        activities = []

        if pd.isna(activity_log):
            return activities

        try:
            # åˆ†å‰²å¤šä¸ªæ´»åŠ¨è®°å½•
            activity_records = activity_log.split("#")

            for record in activity_records:
                if record.strip():
                    parts = record.split(":")
                    if len(parts) >= 5:
                        activities.append(
                            {
                                "item_id": parts[0],
                                "category_id": parts[1],
                                "brand_id": parts[2],
                                "timestamp": parts[3],
                                "action_type": parts[4],
                            }
                        )
        except Exception as e:
            print(f"âš ï¸ è§£æactivity_logå¤±è´¥: {e}")

        return activities

    def _calculate_global_stats(self, interaction_data, y_train):
        """è®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯"""
        print("ğŸ“Š è®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯...")

        if y_train is not None:
            # åŸºäºæ ‡ç­¾è®¡ç®—å…¨å±€å¤è´­ç‡
            total_samples = len(y_train)
            repurchase_samples = sum(y_train == 1)
            global_repurchase_rate = repurchase_samples / total_samples if total_samples > 0 else 0.3
        else:
            # åŸºäºäº¤äº’æ•°æ®æ¨æ–­å¤è´­ç‡
            repurchase_count = sum(1 for inter in interaction_data if inter["purchase_count"] >= 2)
            global_repurchase_rate = repurchase_count / len(interaction_data) if interaction_data else 0.3

        # ç”¨æˆ·å’Œå•†æˆ·çš„å…¨å±€ç»Ÿè®¡
        all_users = set(inter["user_id"] for inter in interaction_data)
        all_merchants = set(inter["merchant_id"] for inter in interaction_data)

        self.global_stats = {
            "global_repurchase_rate": global_repurchase_rate,
            "global_user_repurchase_rate": global_repurchase_rate,  # ç”¨æˆ·ç»´åº¦
            "global_merchant_repurchase_rate": global_repurchase_rate,  # å•†æˆ·ç»´åº¦
            "total_users": len(all_users),
            "total_merchants": len(all_merchants),
            "total_interactions": len(interaction_data),
        }

        print(f"  âœ… å…¨å±€å¤è´­ç‡: {global_repurchase_rate:.4f}")

    def _calculate_user_priors(self, interaction_data, y_train):
        """è®¡ç®—ç”¨æˆ·å¤è´­å…ˆéªŒæ¦‚ç‡"""
        print("ğŸ“Š è®¡ç®—ç”¨æˆ·å¤è´­å…ˆéªŒæ¦‚ç‡...")

        user_stats = defaultdict(
            lambda: {
                "merchants_visited": set(),
                "total_purchases": 0,
                "repurchase_merchants": set(),
                "categories_purchased": set(),
                "brands_purchased": set(),
                "purchase_intervals": [],
                "avg_purchase_per_merchant": 0,
            }
        )

        # ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·çš„è¡Œä¸º
        for inter in interaction_data:
            user_id = inter["user_id"]
            merchant_id = inter["merchant_id"]
            purchases = inter["purchases"]

            user_stats[user_id]["merchants_visited"].add(merchant_id)
            user_stats[user_id]["total_purchases"] += inter["purchase_count"]

            if inter["purchase_count"] >= 2:
                user_stats[user_id]["repurchase_merchants"].add(merchant_id)

            # æ”¶é›†å“ç±»å’Œå“ç‰Œä¿¡æ¯
            for purchase in purchases:
                user_stats[user_id]["categories_purchased"].add(purchase["category_id"])
                user_stats[user_id]["brands_purchased"].add(purchase["brand_id"])

        # è®¡ç®—ç”¨æˆ·å…ˆéªŒç‰¹å¾
        self.user_prior_stats = {}

        for user_id, stats in user_stats.items():
            merchants_count = len(stats["merchants_visited"])
            repurchase_merchants_count = len(stats["repurchase_merchants"])

            if merchants_count >= self.min_user_interactions:
                # ç”¨æˆ·å¤è´­å€¾å‘ = åœ¨å¤šå°‘æ¯”ä¾‹çš„å•†æˆ·å‘ç”Ÿäº†å¤è´­
                user_repurchase_tendency = repurchase_merchants_count / merchants_count

                # è´å¶æ–¯å¹³æ»‘
                smoothed_tendency = (
                    user_repurchase_tendency * merchants_count
                    + self.global_stats["global_user_repurchase_rate"] * self.pseudo_count
                ) / (merchants_count + self.pseudo_count)

                # ç”¨æˆ·å¤šæ ·æ€§ç‰¹å¾
                category_diversity = len(stats["categories_purchased"])
                brand_diversity = len(stats["brands_purchased"])

                # ç”¨æˆ·æ´»è·ƒåº¦
                avg_purchases_per_merchant = stats["total_purchases"] / merchants_count

                self.user_prior_stats[user_id] = {
                    "user_repurchase_tendency": smoothed_tendency,
                    "user_merchant_diversity": merchants_count,
                    "user_category_diversity": category_diversity,
                    "user_brand_diversity": brand_diversity,
                    "user_avg_purchases_per_merchant": avg_purchases_per_merchant,
                    "user_total_purchases": stats["total_purchases"],
                    "user_sample_size": merchants_count,
                }

        print(f"  âœ… è®¡ç®—äº† {len(self.user_prior_stats)} ä¸ªç”¨æˆ·çš„å…ˆéªŒæ¦‚ç‡")

    def _calculate_merchant_priors(self, interaction_data, y_train):
        """è®¡ç®—å•†æˆ·è¢«å¤è´­å…ˆéªŒæ¦‚ç‡"""
        print("ğŸ“Š è®¡ç®—å•†æˆ·è¢«å¤è´­å…ˆéªŒæ¦‚ç‡...")

        merchant_stats = defaultdict(
            lambda: {
                "users_served": set(),
                "total_purchases": 0,
                "repurchase_users": set(),
                "categories_offered": set(),
                "brands_offered": set(),
                "items_offered": set(),
                "user_purchase_counts": [],
            }
        )

        # ç»Ÿè®¡æ¯ä¸ªå•†æˆ·çš„è¡Œä¸º
        for inter in interaction_data:
            user_id = inter["user_id"]
            merchant_id = inter["merchant_id"]
            purchases = inter["purchases"]

            merchant_stats[merchant_id]["users_served"].add(user_id)
            merchant_stats[merchant_id]["total_purchases"] += inter["purchase_count"]
            merchant_stats[merchant_id]["user_purchase_counts"].append(inter["purchase_count"])

            if inter["purchase_count"] >= 2:
                merchant_stats[merchant_id]["repurchase_users"].add(user_id)

            # æ”¶é›†å•†æˆ·çš„äº§å“ç»„åˆä¿¡æ¯
            for purchase in purchases:
                merchant_stats[merchant_id]["categories_offered"].add(purchase["category_id"])
                merchant_stats[merchant_id]["brands_offered"].add(purchase["brand_id"])
                merchant_stats[merchant_id]["items_offered"].add(purchase["item_id"])

        # è®¡ç®—å•†æˆ·å…ˆéªŒç‰¹å¾
        self.merchant_prior_stats = {}

        for merchant_id, stats in merchant_stats.items():
            users_count = len(stats["users_served"])
            repurchase_users_count = len(stats["repurchase_users"])

            if users_count >= self.min_merchant_interactions:
                # å•†æˆ·è¢«å¤è´­ç‡ = å¤è´­ç”¨æˆ·æ•° / æ€»ç”¨æˆ·æ•°
                merchant_repurchase_rate = repurchase_users_count / users_count

                # è´å¶æ–¯å¹³æ»‘
                smoothed_rate = (
                    merchant_repurchase_rate * users_count
                    + self.global_stats["global_merchant_repurchase_rate"] * self.pseudo_count
                ) / (users_count + self.pseudo_count)

                # å•†æˆ·äº§å“ç»„åˆå¤šæ ·æ€§
                category_diversity = len(stats["categories_offered"])
                brand_diversity = len(stats["brands_offered"])
                item_diversity = len(stats["items_offered"])

                # å•†æˆ·ä¸“ä¸šåŒ–ç¨‹åº¦ï¼ˆåˆ†ç±»è¶Šå°‘è¶Šä¸“ä¸šï¼‰
                specialization_score = 1 / (1 + category_diversity / 5)

                # ç”¨æˆ·ç²˜æ€§ï¼ˆå¹³å‡æ¯ç”¨æˆ·è´­ä¹°æ¬¡æ•°ï¼‰
                avg_purchases_per_user = stats["total_purchases"] / users_count

                # è´­ä¹°é›†ä¸­åº¦ï¼ˆåŸºå°¼ç³»æ•°ï¼‰
                purchase_counts = stats["user_purchase_counts"]
                purchase_concentration = self._calculate_gini_coefficient(purchase_counts)

                self.merchant_prior_stats[merchant_id] = {
                    "merchant_repurchase_rate": smoothed_rate,
                    "merchant_user_count": users_count,
                    "merchant_category_diversity": category_diversity,
                    "merchant_brand_diversity": brand_diversity,
                    "merchant_item_diversity": item_diversity,
                    "merchant_specialization_score": specialization_score,
                    "merchant_avg_purchases_per_user": avg_purchases_per_user,
                    "merchant_purchase_concentration": purchase_concentration,
                    "merchant_total_purchases": stats["total_purchases"],
                    "merchant_sample_size": users_count,
                }

        print(f"  âœ… è®¡ç®—äº† {len(self.merchant_prior_stats)} ä¸ªå•†æˆ·çš„å…ˆéªŒæ¦‚ç‡")

    def _calculate_gini_coefficient(self, values):
        """è®¡ç®—åŸºå°¼ç³»æ•°ï¼Œè¡¡é‡åˆ†å¸ƒçš„ä¸å‡åŒ€ç¨‹åº¦"""
        if not values:
            return 0

        values = sorted(values)
        n = len(values)
        index = np.arange(1, n + 1)
        return (2 * np.sum(index * values)) / (n * np.sum(values)) - (n + 1) / n

    def _generate_feature_dataframes(self, X_train):
        """ç”Ÿæˆç‰¹å¾DataFrame"""
        print("ğŸ“Š ç”Ÿæˆç‰¹å¾DataFrame...")

        # ç”Ÿæˆç”¨æˆ·å…ˆéªŒç‰¹å¾
        user_features_list = []
        all_users = X_train["user_id"].unique()

        for user_id in all_users:
            if user_id in self.user_prior_stats:
                stats = self.user_prior_stats[user_id]
                feature_row = {"user_id": user_id}
                for key, value in stats.items():
                    feature_row[f"user_prev_{key}"] = value
                user_features_list.append(feature_row)
            else:
                # æ–°ç”¨æˆ·æˆ–ä½é¢‘ç”¨æˆ·ï¼Œä½¿ç”¨å…¨å±€å¹³å‡å€¼
                user_features_list.append(
                    {
                        "user_id": user_id,
                        "user_prev_user_repurchase_tendency": self.global_stats["global_user_repurchase_rate"],
                        "user_prev_user_merchant_diversity": 1,
                        "user_prev_user_category_diversity": 1,
                        "user_prev_user_brand_diversity": 1,
                        "user_prev_user_avg_purchases_per_merchant": 1.0,
                        "user_prev_user_total_purchases": 1,
                        "user_prev_user_sample_size": 1,
                    }
                )

        self.user_prior_features = pd.DataFrame(user_features_list)

        # ç”Ÿæˆå•†æˆ·å…ˆéªŒç‰¹å¾
        merchant_features_list = []
        all_merchants = X_train["merchant_id"].unique()

        for merchant_id in all_merchants:
            if merchant_id in self.merchant_prior_stats:
                stats = self.merchant_prior_stats[merchant_id]
                feature_row = {"merchant_id": merchant_id}
                for key, value in stats.items():
                    feature_row[f"merchant_prev_{key}"] = value
                merchant_features_list.append(feature_row)
            else:
                # æ–°å•†æˆ·æˆ–ä½é¢‘å•†æˆ·ï¼Œä½¿ç”¨å…¨å±€å¹³å‡å€¼
                merchant_features_list.append(
                    {
                        "merchant_id": merchant_id,
                        "merchant_prev_merchant_repurchase_rate": self.global_stats["global_merchant_repurchase_rate"],
                        "merchant_prev_merchant_user_count": 1,
                        "merchant_prev_merchant_category_diversity": 1,
                        "merchant_prev_merchant_brand_diversity": 1,
                        "merchant_prev_merchant_item_diversity": 1,
                        "merchant_prev_merchant_specialization_score": 0.5,
                        "merchant_prev_merchant_avg_purchases_per_user": 1.0,
                        "merchant_prev_merchant_purchase_concentration": 0.5,
                        "merchant_prev_merchant_total_purchases": 1,
                        "merchant_prev_merchant_sample_size": 1,
                    }
                )

        self.merchant_prior_features = pd.DataFrame(merchant_features_list)

        print(f"  âœ… ç”¨æˆ·ç‰¹å¾: {self.user_prior_features.shape}")
        print(f"  âœ… å•†æˆ·ç‰¹å¾: {self.merchant_prior_features.shape}")

    def _cache_results(self):
        """ç¼“å­˜ç»“æœ"""
        if self.enable_cache:
            print("ğŸ’¾ ç¼“å­˜å…ˆéªŒç‰¹å¾...")
            os.makedirs(os.path.dirname(self.cache_path), exist_ok=True)

            cached_data = {
                "user_prior_features": self.user_prior_features,
                "merchant_prior_features": self.merchant_prior_features,
                "global_stats": self.global_stats,
            }

            with open(self.cache_path, "wb") as f:
                pickle.dump(cached_data, f)
            print(f"âœ… å…ˆéªŒç‰¹å¾å·²ç¼“å­˜åˆ°: {self.cache_path}")

    def get_feature_names(self):
        """è·å–æ‰€æœ‰ç‰¹å¾åç§°"""
        if not self.is_fitted:
            raise ValueError("Transformer has not been fitted yet.")

        feature_names = []

        if self.user_prior_features is not None:
            feature_names.extend([col for col in self.user_prior_features.columns if col != "user_id"])

        if self.merchant_prior_features is not None:
            feature_names.extend([col for col in self.merchant_prior_features.columns if col != "merchant_id"])

        return feature_names

    def get_stats_summary(self):
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        if not self.is_fitted:
            return None

        summary = {
            "global_stats": self.global_stats,
            "user_prior_count": len(self.user_prior_stats) if self.user_prior_stats else 0,
            "merchant_prior_count": len(self.merchant_prior_stats) if self.merchant_prior_stats else 0,
        }

        if self.user_prior_features is not None:
            summary["user_features_shape"] = self.user_prior_features.shape

        if self.merchant_prior_features is not None:
            summary["merchant_features_shape"] = self.merchant_prior_features.shape

        return summary


if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    import numpy as np

    np.random.seed(42)
    n_samples = 1000

    # æ¨¡æ‹Ÿactivity_logæ•°æ®
    def generate_activity_log():
        activities = []
        n_activities = np.random.randint(1, 5)
        for _ in range(n_activities):
            item_id = f"item_{np.random.randint(1, 100)}"
            cat_id = f"cat_{np.random.randint(1, 10)}"
            brand_id = f"brand_{np.random.randint(1, 20)}"
            timestamp = f"2023{np.random.randint(1, 13):02d}{np.random.randint(1, 29):02d}"
            action_type = np.random.choice(["1", "2", "3"], p=[0.3, 0.5, 0.2])  # 1=è´­ä¹°
            activities.append(f"{item_id}:{cat_id}:{brand_id}:{timestamp}:{action_type}")
        return "#".join(activities)

    test_data = {
        "user_id": np.random.randint(1, 200, n_samples),
        "merchant_id": np.random.randint(1, 50, n_samples),
        "age_range": np.random.randint(1, 6, n_samples),
        "gender": np.random.randint(1, 3, n_samples),
        "activity_log": [generate_activity_log() for _ in range(n_samples)],
    }

    df = pd.DataFrame(test_data)
    X = df
    y = pd.Series(np.random.randint(0, 2, n_samples))  # æ¨¡æ‹Ÿæ ‡ç­¾

    print("ğŸ§ª ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•...")
    print(f"æ¨¡æ‹Ÿæ•°æ®å½¢çŠ¶: {X.shape}")

    transformer = PrevTransformer(min_user_interactions=2, min_merchant_interactions=3, enable_cache=False)

    transformer.fit(X, y)
    transformed_df = transformer.transform(X)

    print(f"è½¬æ¢åçš„æ•°æ®å½¢çŠ¶: {transformed_df.shape}")
    print(f"ç‰¹å¾åç§°: {transformer.get_feature_names()}")
    print(f"ç»Ÿè®¡æ‘˜è¦: {transformer.get_stats_summary()}")
    print(f"è½¬æ¢åçš„æ•°æ®é¢„è§ˆ:\n{transformed_df.head()}")
